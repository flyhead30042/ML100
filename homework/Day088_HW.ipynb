{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work\n",
    "1. 請嘗試寫一個 callback 用來記錄各類別在訓練過程中，對驗證集的 True Positive 與 True Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "\n",
    "# Disable GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 資料前處理\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "def preproc_x(x):\n",
    "    x = x.astype(\"float32\")\n",
    "    x = x.reshape(x.shape[0],-1)\n",
    "    x = scaler.fit_transform(x)\n",
    "    return x\n",
    "\n",
    "def preproc_y(y):\n",
    "    y = keras.utils.to_categorical(y, num_classes=10)\n",
    "    return y      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "建立你的神經網路\n",
    "\"\"\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "def build_mlp(input_dim, layer_output_units, output_units, regularizer=None, dropout_ratio=None, isBN=False):\n",
    "    model = Sequential()\n",
    "    for i, untis in enumerate(layer_output_units):  \n",
    "        if i == 0:\n",
    "            model.add(Dense(units=untis, \n",
    "                            input_dim=input_dim,\n",
    "                            kernel_initializer='normal',\n",
    "                            kernel_regularizer=regularizer, \n",
    "                            activation='relu'))\n",
    "            if isBN:\n",
    "                model.add(BatchNormalization())\n",
    "        else:\n",
    "            model.add(Dense(units=untis,\n",
    "                            kernel_initializer='normal',\n",
    "                            kernel_regularizer=regularizer, \n",
    "                            activation='relu'))\n",
    "            if isBN:\n",
    "                model.add(BatchNormalization())\n",
    "\n",
    "    if dropout_ratio:\n",
    "        model.add(Dropout(dropout_ratio))\n",
    "        \n",
    "    model.add(Dense(units=output_units,\n",
    "                kernel_initializer='normal', \n",
    "                activation='softmax'))\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "訓練模型\n",
    "\"\"\"\n",
    "def complie_train_model(model, optimizer, epochs, batch_size, callbacks,   x_train, y_train, x_test, y_test):\n",
    "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer= optimizer)\n",
    "    model.fit(x_train, y_train, \n",
    "          epochs= epochs, \n",
    "          batch_size=batch_size, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=callbacks, \n",
    "          shuffle=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "以視覺畫方式檢視訓練過程\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_train_history(results, title=\"\"):\n",
    "    for i, result in enumerate(results):\n",
    "        history = result[\"model\"].history.history\n",
    "        tag = \"({})\".format(result[\"tag\"])\n",
    "        df_acc = pd.DataFrame({\"acc\"+tag:history[\"acc\"], \"val_acc\"+tag:history[\"val_acc\"]})\n",
    "        df_loss = pd.DataFrame({\"loss\"+tag:history[\"loss\"], \"val_loss\"+tag:history[\"val_loss\"]})\n",
    "        if i ==0:\n",
    "            df_acc_results = df_acc.copy()\n",
    "            df_loss_results = df_loss.copy()\n",
    "        else:\n",
    "            df_acc_results= pd.concat([df_acc_results, df_acc], axis=1)\n",
    "            df_loss_results= pd.concat([df_loss_results, df_loss], axis=1)  \n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(18,5))\n",
    "    styles = ['bs-','ro-','y^-', 'g--']    \n",
    "    g1 = df_acc_results.plot.line(style=styles, ax =ax[0])\n",
    "    g1.set_xlabel(\"Epoch\")\n",
    "    g1.set_ylabel(\"Acc\") \n",
    "    g1.set_title(title) \n",
    "    \n",
    "    g2 = df_loss_results.plot.line(style=styles, ax =ax[1])\n",
    "    g2.set_xlabel(\"Epoch\")\n",
    "    g2.set_ylabel(\"Loss\") \n",
    "    g2.set_title(title) \n",
    "    return df_acc_results, df_loss_results, g1, g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "設定 callbacks\n",
    "\"\"\"\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def setup_callbacks(ismodelckpt=False, isearlystop=False, isreducelr = False, \n",
    "                    monitor=\"val_acc\", save_only=\"best\", patience=5, min_lr= 1e-12, factor=0.5 ):\n",
    "    cbs=list()\n",
    "    \n",
    "    if ismodelckpt:\n",
    "        modelckpt = ModelCheckpoint(filepath=\"./tmp.h5\",\n",
    "                                    monitor=monitor, \n",
    "                                    save_best_only=\"best\" in save_only,\n",
    "                                    save_weights_only=\"weights\" in save_only)\n",
    "        cbs.append(modelckpt)\n",
    "    if isearlystop:\n",
    "        earlystop = EarlyStopping(monitor=monitor, \n",
    "                                  patience=patience, \n",
    "                                  verbose=1)\n",
    "        cbs.append(earlystop)\n",
    "    if isreducelr:\n",
    "        reducelr = ReduceLROnPlateau(factor=factor, \n",
    "                              min_lr=min_lr, \n",
    "                              monitor=monitor, \n",
    "                              patience=patience, \n",
    "                              verbose=1)\n",
    "        cbs.append(reducelr)\n",
    "    return cbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 Callbacks\n",
    "from keras.callbacks import Callback\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def ravel_cm(cm):\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "    return FP, FN, TP, TN\n",
    "\n",
    "# Record_fp_tp\n",
    "class Record_tp_tn(Callback):\n",
    "    \"\"\"\n",
    "    撰寫一個紀錄 ture positive 與 true negative 數量的 callbacks    \n",
    "    \"\"\"\n",
    "    def on_train_begin(self, epoch, logs = {}):            \n",
    "        if \"val_tp\" not in self.params['metrics']:\n",
    "            self.params['metrics'].append(\"val_tp\")\n",
    "        if \"val_tn\" not in self.params['metrics']:\n",
    "            self.params['metrics'].append(\"val_tn\")            \n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs = {}, thres=0.5):\n",
    "        y_true = self.validation_data[1].argmax(axis = 1)\n",
    "        y_pred = self.model.predict(self.validation_data[0])   \n",
    "        y_pred = (y_pred[:, 1] >= thres) * 1\n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        FP, FN, TP, TN = ravel_cm(cm)\n",
    "        print(TP)\n",
    "        print(TN)\n",
    "        logs[\"val_tp\"] = TP\n",
    "        logs[\"val_tn\"] = TN\n",
    "    \n",
    "rec_tptn = Record_tp_tn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train\n",
    "x_test, y_test = test\n",
    "\n",
    "# Preproc the inputs\n",
    "x_train = preproc_x(x_train)\n",
    "x_test = preproc_x(x_test)\n",
    "\n",
    "# Preprc the outputs\n",
    "y_train = preproc_y(y_train)\n",
    "y_test = preproc_y(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 超參數設定\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 1024\n",
    "MOMENTUM = 0.95\n",
    "DROPOUT_EXP = 0.25\n",
    "L2_EXP = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 20s 407us/step - loss: 36.7838 - acc: 0.1686 - val_loss: 36.1393 - val_acc: 0.2754\n",
      "[1000    5    0    0    0    0    0    0    0    0]\n",
      "[   7 8998 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 14s 288us/step - loss: 35.6621 - acc: 0.2929 - val_loss: 35.0908 - val_acc: 0.3350\n",
      "[1000   39    0    0    0    0    0    0    0    0]\n",
      "[  58 8981 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 13s 257us/step - loss: 34.6431 - acc: 0.3350 - val_loss: 34.1170 - val_acc: 0.3579\n",
      "[1000   94    0    0    0    0    0    0    0    0]\n",
      "[ 132 8962 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 14s 284us/step - loss: 33.6833 - acc: 0.3569 - val_loss: 33.1830 - val_acc: 0.3726loss: 33.99 - ETA: 5s - loss: 33.9088 - acc: 0.35 - ETA: 4s - loss: - ETA: 3s - los\n",
      "[1000  117    0    0    0    0    0    0    0    0]\n",
      "[ 166 8951 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 10s 199us/step - loss: 32.7616 - acc: 0.3704 - val_loss: 32.2753 - val_acc: 0.3855- ETA: 5s - loss: 33.0200 - acc - ETA: 4s - loss: 32.9849 - acc\n",
      "[1000  141    0    0    0    0    0    0    0    0]\n",
      "[ 201 8940 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 9s 188us/step - loss: 31.8555 - acc: 0.3880 - val_loss: 31.3902 - val_acc: 0.3990\n",
      "[999 157   0   0   0   0   0   0   0   0]\n",
      "[ 225 8931 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 9s 189us/step - loss: 30.9797 - acc: 0.3974 - val_loss: 30.5258 - val_acc: 0.4077\n",
      "[1000  162    0    0    0    0    0    0    0    0]\n",
      "[ 235 8927 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 9s 190us/step - loss: 30.1201 - acc: 0.4065 - val_loss: 29.6800 - val_acc: 0.4201\n",
      "[1000  173    0    0    0    0    0    0    0    0]\n",
      "[ 243 8930 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 9s 187us/step - loss: 29.2827 - acc: 0.4164 - val_loss: 28.8524 - val_acc: 0.4280\n",
      "[998 201   0   0   0   0   0   0   0   0]\n",
      "[ 283 8916 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 9s 187us/step - loss: 28.4598 - acc: 0.4221 - val_loss: 28.0437 - val_acc: 0.43247s - loss: 28.8498 - - ETA - ETA: 0s - loss: 28.4743 - acc: 0.\n",
      "[998 213   0   0   0   0   0   0   0   0]\n",
      "[ 295 8916 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 9s 187us/step - loss: 27.6563 - acc: 0.4284 - val_loss: 27.2512 - val_acc: 0.4377\n",
      "[998 236   0   0   0   0   0   0   0   0]\n",
      "[ 332 8902 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 10s 192us/step - loss: 26.8698 - acc: 0.4349 - val_loss: 26.4764 - val_acc: 0.4413loss: 26. - ETA: 1s - loss: 26.93\n",
      "[996 237   0   0   0   0   0   0   0   0]\n",
      "[ 341 8892 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 9s 188us/step - loss: 26.1010 - acc: 0.4410 - val_loss: 25.7178 - val_acc: 0.4459\n",
      "[997 253   0   0   0   0   0   0   0   0]\n",
      "[ 358 8892 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 9s 189us/step - loss: 25.3456 - acc: 0.4478 - val_loss: 24.9749 - val_acc: 0.4489\n",
      "[997 264   0   0   0   0   0   0   0   0]\n",
      "[ 370 8891 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 9s 187us/step - loss: 24.6096 - acc: 0.4521 - val_loss: 24.2493 - val_acc: 0.4515\n",
      "[995 273   0   0   0   0   0   0   0   0]\n",
      "[ 384 8884 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 9s 186us/step - loss: 23.8884 - acc: 0.4551 - val_loss: 23.5394 - val_acc: 0.4564\n",
      "[994 292   0   0   0   0   0   0   0   0]\n",
      "[ 407 8879 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 9s 186us/step - loss: 23.1836 - acc: 0.4610 - val_loss: 22.8442 - val_acc: 0.46000s - loss: 23.2157 - a\n",
      "[994 307   0   0   0   0   0   0   0   0]\n",
      "[ 436 8865 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 22.4920 - acc: 0.4663 - val_loss: 22.1660 - val_acc: 0.4605\n",
      "[994 315   0   0   0   0   0   0   0   0]\n",
      "[ 440 8869 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 10s 193us/step - loss: 21.8173 - acc: 0.4678 - val_loss: 21.5018 - val_acc: 0.46471 - acc:  - ETA: 2s - loss: 21.9149 - acc:  - ETA: 2s - loss: 21.8972 - acc - ETA: 1s - loss: 21.87\n",
      "[994 329   0   0   0   0   0   0   0   0]\n",
      "[ 451 8872 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 14s 286us/step - loss: 21.1581 - acc: 0.4732 - val_loss: 20.8521 - val_acc: 0.4675\n",
      "[994 342   0   0   0   0   0   0   0   0]\n",
      "[ 467 8869 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 10s 196us/step - loss: 20.5146 - acc: 0.4768 - val_loss: 20.2188 - val_acc: 0.4692\n",
      "[995 336   0   0   0   0   0   0   0   0]\n",
      "[ 456 8875 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 9s 190us/step - loss: 19.8853 - acc: 0.4807 - val_loss: 19.6001 - val_acc: 0.4733\n",
      "[995 344   0   0   0   0   0   0   0   0]\n",
      "[ 468 8871 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 10s 194us/step - loss: 19.2692 - acc: 0.4858 - val_loss: 18.9976 - val_acc: 0.4733s - loss: 1 - ETA: 0s - loss: 19.2740 - acc: 0.48\n",
      "[994 360   0   0   0   0   0   0   0   0]\n",
      "[ 489 8865 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 9s 189us/step - loss: 18.6697 - acc: 0.4887 - val_loss: 18.4095 - val_acc: 0.4758\n",
      "[995 349   0   0   0   0   0   0   0   0]\n",
      "[ 475 8869 9000 9000 9000 9000 9000 9000 9000 9000]\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 9s 187us/step - loss: 18.0838 - acc: 0.4925 - val_loss: 17.8326 - val_acc: 0.4769: 0.49 - ETA: 4s - loss: 18. - ETA: 2s - loss: 18.1733 - acc:  - ETA: 2s - l\n",
      "[993 356   0   0   0   0   0   0   0   0]\n",
      "[ 476 8873 9000 9000 9000 9000 9000 9000 9000 9000]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "撰寫你的訓練流程並將結果用 dictionary 紀錄\n",
    "\"\"\"\n",
    "results = list()\n",
    "\n",
    "keras.backend.clear_session() # 把舊的 Graph 清掉\n",
    "regularizer = keras.regularizers.l1_l2(l1=L2_EXP, l2=L2_EXP)\n",
    "model = build_mlp(input_dim=x_train.shape[1], layer_output_units=[256,128], output_units=10, regularizer=regularizer, dropout_ratio=DROPOUT_EXP, isBN=True)\n",
    "optimizer =  keras.optimizers.SGD(lr=LEARNING_RATE, nesterov=True, momentum=MOMENTUM)\n",
    "callbacks = [Record_tp_tn()]\n",
    "model = complie_train_model(model=model, optimizer=optimizer,  epochs=EPOCHS , batch_size=BATCH_SIZE, callbacks=callbacks, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)\n",
    "results.append({\"tag\": \"tptn\", \"model\":model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_tp\n",
      "[1000    5    0    0    0    0    0    0    0    0 1000   39    0    0\n",
      "    0    0    0    0    0    0 1000   94    0    0    0    0    0    0\n",
      "    0    0 1000  117    0    0    0    0    0    0    0    0 1000  141\n",
      "    0    0    0    0    0    0    0    0  999  157    0    0    0    0\n",
      "    0    0    0    0 1000  162    0    0    0    0    0    0    0    0\n",
      " 1000  173    0    0    0    0    0    0    0    0  998  201    0    0\n",
      "    0    0    0    0    0    0  998  213    0    0    0    0    0    0\n",
      "    0    0  998  236    0    0    0    0    0    0    0    0  996  237\n",
      "    0    0    0    0    0    0    0    0  997  253    0    0    0    0\n",
      "    0    0    0    0  997  264    0    0    0    0    0    0    0    0\n",
      "  995  273    0    0    0    0    0    0    0    0  994  292    0    0\n",
      "    0    0    0    0    0    0  994  307    0    0    0    0    0    0\n",
      "    0    0  994  315    0    0    0    0    0    0    0    0  994  329\n",
      "    0    0    0    0    0    0    0    0  994  342    0    0    0    0\n",
      "    0    0    0    0  995  336    0    0    0    0    0    0    0    0\n",
      "  995  344    0    0    0    0    0    0    0    0  994  360    0    0\n",
      "    0    0    0    0    0    0  995  349    0    0    0    0    0    0\n",
      "    0    0  993  356    0    0    0    0    0    0    0    0]\n",
      "val_tn\n",
      "[   7 8998 9000 9000 9000 9000 9000 9000 9000 9000   58 8981 9000 9000\n",
      " 9000 9000 9000 9000 9000 9000  132 8962 9000 9000 9000 9000 9000 9000\n",
      " 9000 9000  166 8951 9000 9000 9000 9000 9000 9000 9000 9000  201 8940\n",
      " 9000 9000 9000 9000 9000 9000 9000 9000  225 8931 9000 9000 9000 9000\n",
      " 9000 9000 9000 9000  235 8927 9000 9000 9000 9000 9000 9000 9000 9000\n",
      "  243 8930 9000 9000 9000 9000 9000 9000 9000 9000  283 8916 9000 9000\n",
      " 9000 9000 9000 9000 9000 9000  295 8916 9000 9000 9000 9000 9000 9000\n",
      " 9000 9000  332 8902 9000 9000 9000 9000 9000 9000 9000 9000  341 8892\n",
      " 9000 9000 9000 9000 9000 9000 9000 9000  358 8892 9000 9000 9000 9000\n",
      " 9000 9000 9000 9000  370 8891 9000 9000 9000 9000 9000 9000 9000 9000\n",
      "  384 8884 9000 9000 9000 9000 9000 9000 9000 9000  407 8879 9000 9000\n",
      " 9000 9000 9000 9000 9000 9000  436 8865 9000 9000 9000 9000 9000 9000\n",
      " 9000 9000  440 8869 9000 9000 9000 9000 9000 9000 9000 9000  451 8872\n",
      " 9000 9000 9000 9000 9000 9000 9000 9000  467 8869 9000 9000 9000 9000\n",
      " 9000 9000 9000 9000  456 8875 9000 9000 9000 9000 9000 9000 9000 9000\n",
      "  468 8871 9000 9000 9000 9000 9000 9000 9000 9000  489 8865 9000 9000\n",
      " 9000 9000 9000 9000 9000 9000  475 8869 9000 9000 9000 9000 9000 9000\n",
      " 9000 9000  476 8873 9000 9000 9000 9000 9000 9000 9000 9000]\n"
     ]
    }
   ],
   "source": [
    "def reduce(arr):\n",
    "    a = arr[0]\n",
    "    for b in arr[1:]:\n",
    "        a = np.append(a , b)\n",
    "    return a\n",
    "    \n",
    "h = results[0]['model'].history.history\n",
    "val_tp = reduce(h['val_tp'])\n",
    "print(\"val_tp\")\n",
    "print(val_tp)\n",
    "val_tn = reduce(h['val_tn'])\n",
    "print(\"val_tn\")\n",
    "print(val_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
